# benchmark_module.py

"""
–ú–æ–¥—É–ª—å –¥–ª—è —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ Whisper –∏ Faster-Whisper
–í–∫–ª—é—á–∞–µ—Ç –∑–∞–º–µ—Ä—ã –≤—Ä–µ–º–µ–Ω–∏, –ø–∞–º—è—Ç–∏ –∏ –º–µ—Ç—Ä–∏–∫–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ (WER, CER)
"""

import time
import psutil
import os
from typing import Callable, Dict, List, Optional
from dataclasses import dataclass
import jiwer  # pip install jiwer
from pathlib import Path


@dataclass
class BenchmarkResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–∞"""
    model_name: str
    load_time: float
    transcribe_time: float
    total_time: float
    memory_used_mb: float
    cpu_percent: float
    transcription: str
    wer: Optional[float] = None  # Word Error Rate
    cer: Optional[float] = None  # Character Error Rate


class ASRBenchmark:
    """–ö–ª–∞—Å—Å –¥–ª—è –±–µ–Ω—á–º–∞—Ä–∫–∏–Ω–≥–∞ ASR —Å–∏—Å—Ç–µ–º"""
    
    def __init__(self, audio_path: str, reference_text: Optional[str] = None):
        """
        Args:
            audio_path: –ø—É—Ç—å –∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª—É –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            reference_text: —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ WER/CER (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        self.audio_path = audio_path
        self.reference_text = reference_text
        self.results: List[BenchmarkResult] = []
    
    def benchmark_function(
        self,
        transcribe_func: Callable,
        model_name: str,
        **kwargs
    ) -> BenchmarkResult:
        """
        –ó–∞–º–µ—Ä –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ñ—É–Ω–∫—Ü–∏–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
        
        Args:
            transcribe_func: —Ñ—É–Ω–∫—Ü–∏—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
            model_name: –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
            **kwargs: –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏
        
        Returns:
            BenchmarkResult —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
        """
        
        # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º –Ω–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–∞–º—è—Ç–∏
        process = psutil.Process(os.getpid())
        mem_before = process.memory_info().rss / 1024 / 1024  # MB
        
        print(f"\n{'='*60}")
        print(f"–ë–µ–Ω—á–º–∞—Ä–∫: {model_name}")
        print(f"{'='*60}")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é
        start_cpu = time.time()
        cpu_percent_start = psutil.cpu_percent(interval=None)
        
        result = transcribe_func(self.audio_path, **kwargs)
        
        cpu_percent_end = psutil.cpu_percent(interval=None)
        
        # –ó–∞–º–µ—Ä—è–µ–º –ø–∞–º—è—Ç—å –ø–æ—Å–ª–µ
        mem_after = process.memory_info().rss / 1024 / 1024  # MB
        memory_used = mem_after - mem_before
        
        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏, –µ—Å–ª–∏ –µ—Å—Ç—å —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        wer, cer = None, None
        if self.reference_text:
            wer = self.calculate_wer(self.reference_text, result["text"])
            cer = self.calculate_cer(self.reference_text, result["text"])
            print(f"\nüìä –ú–µ—Ç—Ä–∏–∫–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏:")
            print(f"   WER (Word Error Rate): {wer:.2%}")
            print(f"   CER (Character Error Rate): {cer:.2%}")
        
        benchmark_result = BenchmarkResult(
            model_name=model_name,
            load_time=result["load_time"],
            transcribe_time=result["transcribe_time"],
            total_time=result["total_time"],
            memory_used_mb=memory_used,
            cpu_percent=(cpu_percent_start + cpu_percent_end) / 2,
            transcription=result["text"],
            wer=wer,
            cer=cer
        )
        
        self.results.append(benchmark_result)
        
        print(f"\n‚è±Ô∏è  –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:")
        print(f"   –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {benchmark_result.load_time:.2f} —Å–µ–∫")
        print(f"   –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è: {benchmark_result.transcribe_time:.2f} —Å–µ–∫")
        print(f"   –û–±—â–µ–µ –≤—Ä–µ–º—è: {benchmark_result.total_time:.2f} —Å–µ–∫")
        print(f"   –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –ø–∞–º—è—Ç–∏: {benchmark_result.memory_used_mb:.1f} MB")
        print(f"   –ó–∞–≥—Ä—É–∑–∫–∞ CPU: {benchmark_result.cpu_percent:.1f}%")
        
        return benchmark_result
    
    @staticmethod
    def calculate_wer(reference: str, hypothesis: str) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ Word Error Rate"""
        return jiwer.wer(reference, hypothesis)
    
    @staticmethod
    def calculate_cer(reference: str, hypothesis: str) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ Character Error Rate"""
        return jiwer.cer(reference, hypothesis)
    
    def print_comparison(self):
        """–ü–µ—á–∞—Ç—å —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        if len(self.results) < 2:
            print("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
            return
        
        print(f"\n{'='*80}")
        print("üìä –°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó")
        print(f"{'='*80}\n")
        
        for i, result in enumerate(self.results, 1):
            print(f"{i}. {result.model_name}")
            print(f"   –í—Ä–µ–º—è: {result.total_time:.2f}—Å (–∑–∞–≥—Ä—É–∑–∫–∞: {result.load_time:.2f}—Å, —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è: {result.transcribe_time:.2f}—Å)")
            print(f"   –ü–∞–º—è—Ç—å: {result.memory_used_mb:.1f} MB")
            if result.wer is not None:
                print(f"   –¢–æ—á–Ω–æ—Å—Ç—å: WER={result.wer:.2%}, CER={result.cer:.2%}")
            print()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–±–µ–¥–∏—Ç–µ–ª–µ–π
        fastest = min(self.results, key=lambda x: x.total_time)
        print(f"üèÜ –°–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π: {fastest.model_name} ({fastest.total_time:.2f} —Å–µ–∫)")
        
        if self.results[0].wer is not None:
            most_accurate = min(self.results, key=lambda x: x.wer)
            print(f"üéØ –°–∞–º—ã–π —Ç–æ—á–Ω—ã–π: {most_accurate.model_name} (WER={most_accurate.wer:.2%})")
        
        print(f"\n{'='*80}")


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    from faster_whisper_transcribe import transcribe_with_faster_whisper
    from whisper_transcribe import transcribe_with_whisper
    
    # –ü—É—Ç—å –∫ —Ç–µ—Å—Ç–æ–≤–æ–º—É –∞—É–¥–∏–æ
    audio_file = "test_audio.wav"
    
    # –≠—Ç–∞–ª–æ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (–µ—Å–ª–∏ –µ—Å—Ç—å) –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏
    reference_text = "–ü—Ä–∏–≤–µ—Ç –º–∏—Ä —ç—Ç–æ —Ç–µ—Å—Ç–æ–≤–æ–µ –∞—É–¥–∏–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ"
    
    # –°–æ–∑–¥–∞–µ–º –±–µ–Ω—á–º–∞—Ä–∫
    benchmark = ASRBenchmark(audio_file, reference_text=reference_text)
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º Faster-Whisper
    benchmark.benchmark_function(
        transcribe_with_faster_whisper,
        model_name="Faster-Whisper (base, int8, CPU)",
        model_size="base",
        device="cpu",
        compute_type="int8"
    )
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π Whisper
    benchmark.benchmark_function(
        transcribe_with_whisper,
        model_name="Whisper OpenAI (base, CPU)",
        model_size="base",
        device="cpu"
    )
    
    # –í—ã–≤–æ–¥–∏–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
    benchmark.print_comparison()